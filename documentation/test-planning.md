# Evaluate
## 1. Auto-Scaling Functionality
### Scenario 1: Gradual Load Increase & Decrease
#### Method:
- start with zero or minimal messages in the queue.
- Gradually increase the message production rate (e.g., using a custom message producer application or a tool like locust sending messages to your API endpoint that then enqueues them).
- Observe KEDA scaling up your consumer pods.
- Monitor pod creation time, CPU/memory usage of new pods, and message processing rate.
- Once the system is saturated, gradually decrease the message production rate.
- Observe KEDA scaling down your consumer pods.

#### Metrics:
- Number of consumer pods at different queue depths.
- Time to scale up/down
- Average message processing latency/throughput at each scaling step

### Scenario 2: Sudden Load Spikes
#### Method:
- Suddenly inject a very large number of messages into the queue using locust (e.g., 10,000 or 100,000 messages in a very short burst).
- Observe how quickly KEDA reacts and scales up.
- Monitor for any message backlogs or increased latency during the spike.
#### Metrics:
- Time to reach peak scaling
- Maximum queue depth observed during the spike
- Message processing rate during the spike
- Error rates

### Scenario 3: KEDA Configuration Impact:
#### Method:
- experiment with different parameter settings 
- repeat the above scenarios with these different configurations.


### Configuration Requirements
- locust test results to prometheus
- kubernetes pod metrics result to prometheus
- rabbitmq queue metrics result to prometheus.
- logging start time and completed time of each task in the database to measure latency.

## 2. Fault Tolerance & Resiliency
To test the system's ability to withstand failures.
### Scenario 1: RabbitMQ Node Failure (if we have a rabbitmq cluster)
#### Method:
- kill one of the rabbitMQ pod/nodes while messages are being processed.
- Observe if consumers automatically reconnect another node. 
- check for message loss or duplication
- Bring the node back up and observe cluster recovery.

#### Metrics:
- time for consumers to reconnect.
- Any interruption in message processing duing the failure
- number of re-delivered or duplicated messages:  
- RabbitMQ cluster health status

### Scenario 2: Consumer Pod failure.
#### Method
- during active message processing, randomly kill several consumer pods.
- observe kubernetes' ability to reschedule new pods.
- ensure messages being processed by the killed pods are re-queued and processed by other consumers.
#### Metrics:
- time for new pods to become ready and start processing messages.
- Any message duplication or loss due to consumer failure


> With the current implementation, the queue is generated by the first consumer pod processing it. And then, if the pod dies, then the queue is still there open. Task is not acknowledged. The other pod picks up the unfinished task and processes it and uses the same queue and start sending status from 0% again. 


### Scenario 3: Injecting "Poison Pill" messages.
#### Method: 
- Create messages that are malformed, unparseable, or cause your consumer application to crash (e.g., invalid JSON, missing required fields, division by zero scenarios).
- Inject these messages into the queue.
- Observe how your consumers handle them: do they crash? Do they move to a Dead Letter Queue (DLQ)? Do they retry endlessly?
- Implement and test your DLQ strategy.

#### Metrics:
- number of messages moved to DLQ.
- consumer pod restarts due to "posion pills".
- latency impact on other "good" messages.
"Effectiveness of your error handling and DLQ mechanism.

### Scenario 4: Network Interruption
#### Method:
- Option 1: Kubernetes newtork policies: temporaily apply network polices to block traffic between consumer pods and RabbitMQ
- Option 2: Chaos Mesh / Litmus Chaos: these tools are excellent for injecting network chaos, and other types of failures. We can simular latency, packet loss, and so on.
#### Metrics:
- Message processing stall time
- Message duplication/loss during interruption.


# Thesis Structure for Evaluation

**Testbed Setup**: Describe your Kubernetes cluster (local, cloud, number of nodes), RabbitMQ deployment (single node, cluster), consumer/producer applications, KEDA configuration, and monitoring tools.


**Methodology for Each Test:**
- Objective of the test.
- Specific steps taken (e.g., "Produced X messages/second using Locust," "Killed rabbitmq-0 pod").
- Tools used.
- Metrics collected.

**Results & Analysis:**
- Present your findings, often using graphs and tables.
- Interpret the data: what do the metrics tell you about the system's behavior?
- Discuss observed strengths and weaknesses.
- Compare performance under different conditions or configurations.


**Discussion/Conclusion:** Summarize the overall performance and fault tolerance characteristics of your system. Relate your findings back to your thesis objectives.